"""
MultiAgentCoordinator - Coordinates all agents in the system.

This coordinator uses browser-use Agent for actual browser interaction
while our agents provide enhanced perception, reflection, and planning.
"""

import asyncio
import logging
from typing import Any, Dict, List, Optional
from pathlib import Path

from browser_use import BrowserSession, BrowserProfile, Agent
from browser_use.agent.prompts import AgentMessagePrompt
from browser_use.browser.views import BrowserStateSummary
from browser_use.agent.views import AgentOutput

from .agent_message import EventBus, MessageType
from .shared_memory import SharedMemory, MemoryKey, ContextHints
from .browser_adapter import BrowserAdapter, create_browser_session, BrowserState
from .sequential_thinking import SequentialThinkingEngine, ThinkingContext

logger = logging.getLogger(__name__)


# =============================================================================
# ÐŸÐÐ¢Ð§ Ð”Ð›Ð¯ Ð£Ð’Ð•Ð›Ð˜Ð§Ð•ÐÐ˜Ð¯ Ð›Ð˜ÐœÐ˜Ð¢Ð Ð˜ÐÐ¤ÐžÐ ÐœÐÐ¦Ð˜Ð˜ Ðž Ð¡Ð¢Ð ÐÐÐ˜Ð¦Ð•
# =============================================================================

_original_agent_message_prompt_init = AgentMessagePrompt.__init__

def _patched_agent_message_prompt_init(self, *args, **kwargs):
    """ÐŸÐ°Ñ‚Ñ‡ÐµÐ½Ñ‹Ð¹ __init__ Ñ ÑƒÐ²ÐµÐ»Ð¸Ñ‡ÐµÐ½Ð½Ñ‹Ð¼ Ð»Ð¸Ð¼Ð¸Ñ‚Ð¾Ð¼ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ð¾ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹."""
    if 'max_clickable_elements_length' not in kwargs:
        kwargs['max_clickable_elements_length'] = 150000  # 150K Ð²Ð¼ÐµÑÑ‚Ð¾ 40K
    _original_agent_message_prompt_init(self, *args, **kwargs)

AgentMessagePrompt.__init__ = _patched_agent_message_prompt_init


# =============================================================================
# ÐŸÐÐ¢Ð§ Ð”Ð›Ð¯ Ð˜ÐÐªÐ•ÐšÐ¦Ð˜Ð˜ CONTEXT HINTS Ð’ ÐŸÐ ÐžÐœÐŸÐ¢
# =============================================================================

# Ð“Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ð°Ñ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ð°Ñ Ð´Ð»Ñ Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ñ… ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð½Ñ‹Ñ… Ð¿Ð¾Ð´ÑÐºÐ°Ð·Ð¾Ðº
_current_context_hints: Optional[ContextHints] = None

# Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´
_original_get_user_message = AgentMessagePrompt.get_user_message

def _patched_get_user_message(self, *args, **kwargs):
    """
    ÐŸÐ°Ñ‚Ñ‡ÐµÐ½Ñ‹Ð¹ get_user_message ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¸Ð½ÑŠÐµÐºÑ‚Ð¸Ñ€ÑƒÐµÑ‚ ContextHints Ð² Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚.

    Ð­Ñ‚Ð¾ ÐºÐ»ÑŽÑ‡ÐµÐ²Ð¾Ðµ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ - Ñ‚ÐµÐ¿ÐµÑ€ÑŒ browser-use Agent ÐŸÐžÐ›Ð£Ð§ÐÐ•Ð¢ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð¾Ñ‚ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð²!
    """
    # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ
    original = _original_get_user_message(self, *args, **kwargs)

    # Ð•ÑÐ»Ð¸ Ð½ÐµÑ‚ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð½Ñ‹Ñ… Ð¿Ð¾Ð´ÑÐºÐ°Ð·Ð¾Ðº, Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»
    global _current_context_hints
    if _current_context_hints is None:
        return original

    # Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ ÐœÐ˜ÐÐ˜ÐœÐÐ›Ð˜Ð¡Ð¢Ð˜Ð§ÐÐž
    context_str = _current_context_hints.to_prompt_context()
    if not context_str:
        return original

    # Ð˜Ð½ÑŠÐµÐºÑ‚Ð¸Ñ€ÑƒÐµÐ¼ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð² UserMessage Ð¾Ð±ÑŠÐµÐºÑ‚
    # UserMessage Ð¼Ð¾Ð¶ÐµÑ‚ Ð¸Ð¼ÐµÑ‚ÑŒ content ÐºÐ°Ðº ÑÑ‚Ñ€Ð¾ÐºÑƒ Ð¸Ð»Ð¸ ÑÐ¿Ð¸ÑÐ¾Ðº
    if hasattr(original, 'content'):
        if isinstance(original.content, str):
            # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ðº ÑÑ‚Ñ€Ð¾ÐºÐ¾Ð²Ð¾Ð¼Ñƒ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ñƒ
            original.content = f"{original.content}\n\n{context_str}"
        elif isinstance(original.content, list):
            # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ðº ÑÐ¿Ð¸ÑÐºÑƒ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð° (Ð² Ð¿ÐµÑ€Ð²Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ð¹ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚)
            for item in original.content:
                if hasattr(item, 'text'):
                    item.text = f"{item.text}\n\n{context_str}"
                    break

    return original

# ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ Ð¿Ð°Ñ‚Ñ‡
AgentMessagePrompt.get_user_message = _patched_get_user_message


# =============================================================================
# ÐœÐ˜ÐÐ˜ÐœÐÐ›Ð˜Ð¡Ð¢Ð˜Ð§ÐÐ«Ð™ Ð¡Ð˜Ð¡Ð¢Ð•ÐœÐÐ«Ð™ ÐŸÐ ÐžÐœÐŸÐ¢
# =============================================================================

SYSTEM_PROMPT = """
Ð¢Ñ‹ â€” Ð°Ð²Ñ‚Ð¾Ð½Ð¾Ð¼Ð½Ñ‹Ð¹ AI-Ð°Ð³ÐµÐ½Ñ‚ Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€Ð¾Ð¼.

ÐŸÐ ÐÐ’Ð˜Ð›Ð:
- Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐ¹ Ð·Ð°Ð´Ð°Ñ‡Ñƒ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ autonomously
- ÐžÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°Ð¹ÑÑ ÐºÐ¾Ð³Ð´Ð° Ð·Ð°Ð´Ð°Ñ‡Ð° Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð°
- ÐÐµ ÑÐ¾Ð²ÐµÑ€ÑˆÐ°Ð¹ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½ÑƒÑŽ Ð¾Ð¿Ð»Ð°Ñ‚Ñƒ Ð±ÐµÐ· Ð¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð¶Ð´ÐµÐ½Ð¸Ñ
- Ð Ð°Ð±Ð¾Ñ‚Ð°Ð¹ Ð² Ð¾Ð´Ð½Ð¾Ð¹ Ð²ÐºÐ»Ð°Ð´ÐºÐµ
- ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐµ
"""


class MultiAgentCoordinator:
    """
    Main coordinator for the multi-agent system.

    Uses browser-use Agent for execution while our agents
    provide enhanced capabilities through callbacks.
    """

    def __init__(
        self,
        browser_session: BrowserSession,
        llm: Any,
        max_steps: int = 25,
        debug: bool = False,
    ):
        self.browser_session = browser_session
        self.llm = llm
        self.max_steps = max_steps
        self.debug = debug
        self._logger = logging.getLogger("MultiAgentCoordinator")

        if debug:
            self._logger.setLevel(logging.DEBUG)
            logging.basicConfig(level=logging.DEBUG)

        # Initialize core components
        self.event_bus = EventBus()
        self.shared_memory = SharedMemory()
        self.browser_adapter = BrowserAdapter(browser_session, debug=debug)
        self.thinking_engine = SequentialThinkingEngine(
            self.shared_memory,
            max_steps=max_steps,
            debug=debug,
        )

        # Track execution state
        self._current_step = 0
        self._thinking_context: Optional[ThinkingContext] = None
        self._final_result: Optional[str] = None

        # Import agents for callback processing
        from agents.perception_agent import PerceptionAgent
        from agents.reflection_agent import ReflectionAgent

        self.perception_agent = PerceptionAgent(
            self.event_bus,
            self.shared_memory,
            llm,
            debug=debug,
        )
        self.reflection_agent = ReflectionAgent(
            self.event_bus,
            self.shared_memory,
            llm,
            debug=debug,
        )

    async def run_with_agents(self, task: str) -> str:
        """
        Run a task using browser-use Agent with our enhanced agents.

        Args:
            task: The task to execute

        Returns:
            Final result message
        """
        print("\nðŸ¤– Ð—Ð°Ð¿ÑƒÑÐº Multi-Agent System")

        # Initialize task
        await self.shared_memory.set(MemoryKey.TASK_DESCRIPTION, task)
        await self.shared_memory.set(MemoryKey.TASK_STATUS, "running")
        await self.shared_memory.set(MemoryKey.PROGRESS_SCORE, 0.0)

        # Create thinking context
        self._thinking_context = await self.thinking_engine.create_thinking_context(task)

        try:
            # Create browser-use agent with our callback
            agent = Agent(
                task=task,
                llm=self.llm,
                browser_session=self.browser_session,
                extend_system_message=SYSTEM_PROMPT,
                max_steps=self.max_steps,
                include_attributes=[
                    'aria-label', 'title', 'placeholder', 'name', 'type',
                    'value', 'href', 'id', 'class', 'role', 'aria-modal',
                    'aria-selected', 'aria-checked', 'checked', 'selected',
                    'disabled', 'readonly', 'text-content', 'alt', 'label',
                ],
                register_new_step_callback=self._step_callback,
            )

            # Run the agent
            history = await agent.run()

            # Process final result using final_result() method
            if history:
                result = history.final_result()
                # final_result() Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ str Ð¸Ð»Ð¸ None
                if result and isinstance(result, str):
                    self._final_result = result
                else:
                    self._final_result = 'Ð—Ð°Ð´Ð°Ñ‡Ð° Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð°'

            await self.shared_memory.set(MemoryKey.TASK_STATUS, "completed")

            return self._final_result or 'Ð—Ð°Ð´Ð°Ñ‡Ð° Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð°'

        except Exception as e:
            self._logger.error(f"Error in run_with_agents: {e}")
            await self.shared_memory.set(MemoryKey.TASK_STATUS, "failed")
            raise

    async def _step_callback(
        self,
        browser_state: BrowserStateSummary,
        agent_output: AgentOutput,
        step: int,
    ) -> None:
        """
        Callback called by browser-use Agent after each step.

        This is where our agents process the state and provide insights.
        """
        self._current_step = step
        self._logger.info(f"\n{'='*50}")
        self._logger.info(f"STEP {step}/{self.max_steps}")
        self._logger.info(f"{'='*50}")

        try:
            # Update browser adapter with current state
            await self.browser_adapter.update_from_callback(browser_state, agent_output, step)

            # Get URL from browser session
            url = ""
            try:
                url = await self.browser_session.get_current_page_url() or ""
            except Exception:
                pass  # URL might not be available yet

            await self.shared_memory.set(MemoryKey.CURRENT_URL, url)

            # Create simplified browser state dict for our agents
            browser_state_dict = {
                "url": url,
                "title": "",  # Will be filled if available
                "clickable_elements": [],  # Will be extracted if available
                "is_modal_present": False,
                "pagination_detected": False,
            }

            # 1. Perception: Analyze current state
            if self.debug:
                self._logger.info("[1/2] Perception: Analyzing page...")

            perception_result = await self.perception_agent.process({
                "browser_state": browser_state_dict,
            })

            if perception_result.get("success"):
                perception_data = perception_result.get("perception", {})
                self._log_perception(perception_data)

                # 2. Reflection: Evaluate progress
                if self.debug:
                    self._logger.info("[2/2] Reflection: Evaluating...")

                last_result = self.shared_memory.get(MemoryKey.LAST_ACTION_RESULT)

                reflection_result = await self.reflection_agent.process({
                    "action_result": last_result,
                    "perception": perception_data,
                })

                if reflection_result.get("success"):
                    reflection_data = reflection_result.get("reflection", {})
                    self._log_reflection(reflection_data)

                    # Store action result for next step
                    action_result_dict = {
                        "success": True,
                        "action": str(agent_output.action) if hasattr(agent_output, 'action') else "unknown",
                    }
                    await self.shared_memory.set(MemoryKey.LAST_ACTION_RESULT, action_result_dict)

                # 3. Update global context hints for prompt injection
                # Ð­Ñ‚Ð¾ ÐšÐ›Ð®Ð§Ð•Ð’ÐžÐ• Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ - Ñ‚ÐµÐ¿ÐµÑ€ÑŒ browser-use Agent ÐŸÐžÐ›Ð£Ð§Ð˜Ð¢ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚!
                global _current_context_hints
                hints_dict = self.shared_memory.get(MemoryKey.CONTEXT_HINTS)
                if hints_dict:
                    _current_context_hints = ContextHints.from_dict(hints_dict)
                    if self.debug:
                        self._logger.info(f"  [Context Injection] {len(_current_context_hints.observations)} observations, "
                                        f"{len(_current_context_hints.patterns)} patterns, "
                                        f"{len(_current_context_hints.warnings)} warnings")
                else:
                    _current_context_hints = None

        except Exception as e:
            self._logger.error(f"Error in step callback: {e}")

    def _log_perception(self, perception: Dict[str, Any]) -> None:
        """Log perception summary."""
        if not self.debug:
            return

        page_type = perception.get("page_type", "unknown")
        patterns = perception.get("patterns", [])
        modal = perception.get("modal_detected", False)

        self._logger.info(f"  [Perception] Page type: {page_type}")
        self._logger.info(f"  [Perception] Patterns: {', '.join(patterns) if patterns else 'none'}")
        self._logger.info(f"  [Perception] Modal: {'yes' if modal else 'no'}")

    def _log_reflection(self, reflection: Dict[str, Any]) -> None:
        """Log reflection summary."""
        if not self.debug:
            return

        success = reflection.get("action_successful", True)
        progress = reflection.get("progress_score", 0.0)
        next_action = reflection.get("next_action")

        self._logger.info(f"  [Reflection] Last action: {'success' if success else 'failed'}")
        self._logger.info(f"  [Reflection] Progress: {progress*100:.0f}%")
        if next_action:
            self._logger.info(f"  [Reflection] Next: {next_action}")

    def get_stats(self) -> Dict[str, Any]:
        """Get execution statistics."""
        return {
            "steps": self._current_step,
            "max_steps": self.max_steps,
            "progress": self.shared_memory.get(MemoryKey.PROGRESS_SCORE, 0.0),
            "task_status": self.shared_memory.get(MemoryKey.TASK_STATUS, "unknown"),
        }


async def create_coordinator(
    llm: Any,
    headless: bool = False,
    max_steps: int = 25,
    debug: bool = False,
) -> MultiAgentCoordinator:
    """
    Create a new coordinator with browser session.

    Args:
        llm: The LLM instance to use
        headless: Whether to run browser in headless mode
        max_steps: Maximum number of steps to execute
        debug: Enable debug logging

    Returns:
        MultiAgentCoordinator instance
    """
    browser_session = await create_browser_session(
        headless=headless,
        keep_alive=True,
    )

    return MultiAgentCoordinator(
        browser_session=browser_session,
        llm=llm,
        max_steps=max_steps,
        debug=debug,
    )
